name: Manage EKS Cluster

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'EKS cluster name'
        required: true
        default: 'dotnet-cluster'
        type: string
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - dev
          - staging
          - prod
      action:
        description: 'Action to perform'
        required: true
        type: choice
        options:
          - create
          - destroy
        default: 'create'

jobs:
  create-eks:
    if: github.event.inputs.action == 'create'
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ vars.AWS_ACCOUNT }}:role/GitHubActionsRole
          aws-region: ap-southeast-2

      - name: Install eksctl
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Create EKS cluster
        env:
          CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
          CLUSTER_REGION: ap-southeast-2
        run: |
          if ! eksctl get cluster --name $CLUSTER_NAME --region $CLUSTER_REGION --output json | jq -e '.[] | select(.Status=="ACTIVE")'; then
            eksctl create cluster \
              --name $CLUSTER_NAME \
              --region $CLUSTER_REGION \
              --node-type t3.medium \
              --nodes 2 \
              --nodes-min 2 \
              --nodes-max 4 \
              --managed \
              --with-oidc \
              --ssh-access \
              --ssh-public-key k8s-sydney \
              --vpc-cidr 10.0.0.0/16
          else
            echo "Cluster ${CLUSTER_NAME} already exists in region ${CLUSTER_REGION} and is ACTIVE."
          fi

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ github.event.inputs.cluster_name }} --region ap-southeast-2
          kubectl get nodes

      - name: Install AWS Load Balancer Controller
        run: |
          # Associate OIDC provider
          eksctl utils associate-iam-oidc-provider \
            --cluster ${{ github.event.inputs.cluster_name }} \
            --region ap-southeast-2 \
            --approve

          # Create IAM policy
          if ! aws iam get-policy --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy &>/dev/null; then
            curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json
            aws iam create-policy \
              --policy-name AWSLoadBalancerControllerIAMPolicy \
              --policy-document file://iam-policy.json
          else
            echo "IAM policy already exists, creating new version..."
            curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json
            aws iam create-policy-version \
              --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy \
              --policy-document file://iam-policy.json \
              --set-as-default
          fi

          # Get policy ARN
          POLICY_ARN=$(aws iam get-policy --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy --query 'Policy.Arn' --output text)

          # Create service account with IRSA
          eksctl create iamserviceaccount \
            --cluster=${{ github.event.inputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --attach-policy-arn=$POLICY_ARN \
            --override-existing-serviceaccounts \
            --approve

          # Get VPC ID and public subnets
          VPC_ID=$(aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
          
          # Tag public subnets for LoadBalancer use
          aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" "Name=map-public-ip-on-launch,Values=true" \
            --query 'Subnets[*].SubnetId' --output text | while read subnet; do
            aws ec2 create-tags --resources $subnet --tags Key=kubernetes.io/role,Value=elb
          done

          # Install AWS Load Balancer Controller
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=${{ github.event.inputs.cluster_name }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set vpcId=$VPC_ID \
            --set subnetSelector=kubernetes.io/role=elb \
            --set region=ap-southeast-2

  destroy-eks:
    if: github.event.inputs.action == 'destroy'
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ vars.AWS_ACCOUNT }}:role/GitHubActionsRole
          aws-region: ap-southeast-2

      - name: Install eksctl
        run: |
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin

      - name: Delete EKS cluster
        env:
          CLUSTER_NAME: ${{ github.event.inputs.cluster_name }}
          CLUSTER_REGION: ap-southeast-2
        run: |
          if eksctl get cluster --name $CLUSTER_NAME --region $CLUSTER_REGION --output json | jq -e '.[] | select(.Status=="ACTIVE")'; then
            # Delete the cluster
            eksctl delete cluster --name $CLUSTER_NAME --region $CLUSTER_REGION

            # Delete the IAM policy
            if aws iam get-policy --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy &>/dev/null; then
              # Get all policy versions
              POLICY_VERSIONS=$(aws iam list-policy-versions --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text)
              
              # Delete all non-default versions
              for version in $POLICY_VERSIONS; do
                aws iam delete-policy-version --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy --version-id $version
              done
              
              # Delete the policy
              aws iam delete-policy --policy-arn arn:aws:iam::${{ vars.AWS_ACCOUNT }}:policy/AWSLoadBalancerControllerIAMPolicy
            fi
          else
            echo "Cluster ${CLUSTER_NAME} does not exist in region ${CLUSTER_REGION}."
          fi
